{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Indian Recipe Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(row):\n",
    "    text = row\n",
    "    sections = [part.strip() for part in text.split(\"###\") if part.strip()]\n",
    "    parsed_data = {}\n",
    "    for section in sections:\n",
    "        if \": \" in section:\n",
    "            key, value = section.split(\": \", 1)\n",
    "            parsed_data[key] = value\n",
    "    return parsed_data\n",
    "\n",
    "# Apply the function to each row\n",
    "parsed_df = df[\"text\"].apply(parse_text)\n",
    "\n",
    "# Convert parsed data into a new DataFrame\n",
    "result_df = pd.DataFrame(parsed_df.tolist())\n",
    "\n",
    "\n",
    "df = result_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create empty lists to store the input-output pairs\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "\n",
    "# Iterate through the rows to generate input-output pairs\n",
    "for _, row in df.iterrows():\n",
    "    # Create the input text\n",
    "    input_text = f\"Ingredients: {row['TranslatedIngredients']}; Time: {row['PrepTimeInMins'] + row['CookTimeInMins']} mins; Cuisine: {row['Cuisine']}; Diet: {row['Diet']}\"\n",
    "    \n",
    "    # Create the output text\n",
    "    output_text = f\"Recipe Name: {row['Course']} Recipe; Instructions: {row['TranslatedInstructions']}; Servings: {row['Servings']}\"\n",
    "    \n",
    "    # Append to the lists\n",
    "    input_texts.append(input_text)\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "# Add the input-output pairs to the DataFrame (optional)\n",
    "df['Input'] = input_texts\n",
    "df['Output'] = output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"recipe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df=df[['Input','Output']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model from the checkpoint (not from \"t5-small\")\n",
    "checkpoint_path = './results/checkpoint-1374'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset for Hugging Face\n",
    "data = {'Input': input_texts, 'Output': output_texts}\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(examples['Input'], padding='max_length', truncation=True, max_length=512)\n",
    "    output_encodings = tokenizer(examples['Output'], padding='max_length', truncation=True, max_length=512)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': output_encodings['input_ids'],\n",
    "    }\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and eval datasets (80% train, 20% eval)\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass Trainer to customize the optimizer\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        \"\"\"\n",
    "        Override the default optimizer creation method to use AdamW.\n",
    "        \"\"\"\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.args.learning_rate,\n",
    "            weight_decay=self.args.weight_decay,\n",
    "        )\n",
    "        self.optimizer = optimizer\n",
    "        return self.optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the custom trainer with AdamW\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from checkpoint-687\n",
    "trainer.train(resume_from_checkpoint='./results/checkpoint-1374')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding output as markdown because of large output**\n",
    "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
    "c:\\Users\\abhin\\Desktop\\AI 30 DAYS\\Recipe Generator\\recipeenv\\lib\\site-packages\\transformers\\trainer.py:3420: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
    "  0%|          | 0/2061 [00:00<?, ?it/s]c:\\Users\\abhin\\Desktop\\AI 30 DAYS\\Recipe Generator\\recipeenv\\lib\\site-packages\\transformers\\trainer.py:3083: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
    "  checkpoint_rng_state = torch.load(rng_file)\n",
    "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
    "\n",
    " 68%|██████▊   | 1400/2061 [01:35<05:11,  2.12it/s] \n",
    "{'loss': 1.6009, 'grad_norm': 0.6635178923606873, 'learning_rate': 1.603590490053372e-05, 'epoch': 2.04}\n",
    "\n",
    " 73%|███████▎  | 1500/2061 [07:40<33:57,  3.63s/it]\n",
    "{'loss': 1.5683, 'grad_norm': 0.5967133045196533, 'learning_rate': 1.3609898107714703e-05, 'epoch': 2.18}\n",
    "\n",
    " 78%|███████▊  | 1600/2061 [13:47<27:46,  3.61s/it]\n",
    "{'loss': 1.5441, 'grad_norm': 1.2870875597000122, 'learning_rate': 1.1183891314895683e-05, 'epoch': 2.33}\n",
    "\n",
    " 82%|████████▏ | 1700/2061 [19:54<21:47,  3.62s/it]\n",
    "{'loss': 1.5891, 'grad_norm': 0.5602926015853882, 'learning_rate': 8.757884522076662e-06, 'epoch': 2.47}\n",
    "\n",
    " 87%|████████▋ | 1800/2061 [25:55<15:47,  3.63s/it]\n",
    "{'loss': 1.5901, 'grad_norm': 0.5525727272033691, 'learning_rate': 6.3318777292576415e-06, 'epoch': 2.62}\n",
    "\n",
    " 92%|█████████▏| 1900/2061 [31:57<09:40,  3.61s/it]\n",
    "{'loss': 1.6074, 'grad_norm': 0.43832340836524963, 'learning_rate': 3.905870936438622e-06, 'epoch': 2.77}\n",
    "\n",
    " 97%|█████████▋| 2000/2061 [37:57<04:02,  3.98s/it]\n",
    "{'loss': 1.611, 'grad_norm': 0.4432830214500427, 'learning_rate': 1.4798641436196021e-06, 'epoch': 2.91}\n",
    "\n",
    "                                                   \n",
    "100%|██████████| 2061/2061 [43:33<00:00,  3.65s/it]\n",
    "{'eval_loss': 1.4431242942810059, 'eval_runtime': 109.8543, 'eval_samples_per_second': 12.517, 'eval_steps_per_second': 1.566, 'epoch': 3.0}\n",
    "\n",
    "100%|██████████| 2061/2061 [43:35<00:00,  1.27s/it]\n",
    "{'train_runtime': 2615.314, 'train_samples_per_second': 6.304, 'train_steps_per_second': 0.788, 'train_loss': 0.5302758270071178, 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recipeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
